{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933d3111",
   "metadata": {},
   "source": [
    "A Simple Digit Recognizer from the infamous MNIST dataset. This is actually called the \"Hello World!\" for Neural Networks. The project is taking help from a kaggle note book adn two you tube videos.\n",
    "\n",
    "1. Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math) by Samson Zhang - https://www.youtube.com/watch?v=w8yWXqWQYmU\n",
    "2. Neural networks Series by 3Blue1Brown (First 4 videos are good enough for the project) - https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the needed libraries and connecting to the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data  =pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dd0ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m,n = data.shape\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae8a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There could be a chance the dataset will be ordered, so we shuffle it\n",
    "np.random.shuffle(data) \n",
    "\n",
    "#Splitting the adatset into dev and training sets and transposing it\n",
    "\n",
    "#Dev dataset\n",
    "data_dev=data[1:1000].T\n",
    "Y_dev=data_dev[0]\n",
    "X_dev=data_dev[1:n]\n",
    "# Normalizing the data from 0–255 (grayscale) to 0–1\n",
    "X_dev= X_dev/255.0 \n",
    "\n",
    "#Training dataset\n",
    "data_train=data[1000:m].T\n",
    "Y_train= data_train[0]\n",
    "X_train=data_train[1:n]\n",
    "X_train=X_train/255.0\n",
    "\n",
    "_,m_train=X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f1917d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5e45d",
   "metadata": {},
   "source": [
    "Neural Network Architecture\n",
    "Input Layer a[0]:\n",
    "\n",
    "    784 units\n",
    "\n",
    "    One for each pixel in a 28×28 image\n",
    "\n",
    "Hidden Layer a[1]:\n",
    "\n",
    "    10 units\n",
    "\n",
    "    Uses ReLU activation function\n",
    "\n",
    "Output Layer a[2]:\n",
    "\n",
    "    10 units\n",
    "\n",
    "    One for each digit class (0–9)\n",
    "\n",
    "    Uses Softmax activation to output class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eec3c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the input parameters or initial weight and biases for the neural network\n",
    "\n",
    "def init_params():\n",
    "    W1=np.random.rand(10,784)-0.5 #to keeep the values between 0.5 and -0.5\n",
    "    b1=np.random.rand(10,1)-0.5\n",
    "    W2=np.random.rand(10,10)-0.5\n",
    "    b2=np.random.rand(10,1)-0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "#Lets define the ReLu activation function\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z) #there is still possibility for making it faster- i will come back for this later\n",
    "\n",
    "#Lets define the softmax function- will help convert the activation values into proobabilities\n",
    "def softmax(Z):\n",
    "    A= np.exp(Z)/sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "#Lets define the forward propogation function\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1= np.dot(W1, X) + b1\n",
    "    A1=ReLU(Z1)\n",
    "    Z2= np.dot(W2, A1) + b2\n",
    "    A2= softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "#Lets define the one-hot encoding function\n",
    "def one_hot(Y):\n",
    "    one_hot_Y= np.zeros((Y.size, Y.max()+1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y= one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "#Lets define the ReLU derivative function\n",
    "def ReLU_deriv(Z):\n",
    "    return Z>0\n",
    "\n",
    "#Lets define the backward propogation function\n",
    "def backward_prop(W1, Z1, A1, W2, A2, Z2, X, Y):\n",
    "    one_hot_Y=one_hot(Y)\n",
    "    dZ2= A2- one_hot_Y\n",
    "#Each column of A1 is the activation from the hidden layer for one example, and each column of dZ2 is the error signal for the same example. and hence we transponse A1\n",
    "    dW2=1/m*np.dot(dZ2,A1.T)\n",
    "    db2= 1/m*np.sum(dZ2)\n",
    "    dZ1=np.dot(W2.T,dZ2)*ReLU_deriv(Z1) #called the hadamard product (element wise multiplication to keep the gradient only whre the neuron was active Z1>0\n",
    "    dW1=1/m*np.dot(dZ1,X.T)\n",
    "    db1=1/m*np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "#Lets define the update parameters function, also passing thhe hyper parameter learning rate- alpha\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1= W1-alpha*dW1\n",
    "    b1= b1-alpha*db1\n",
    "    W2= W2-alpha*dW2\n",
    "    b2= b2-alpha*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04c02591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the get predictions function\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=0)  #returns the index of the maximum value in each column of A2, which corresponds to the predicted class\n",
    "\n",
    "#defining the accuracy function\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions== Y) / Y.size  #calculates the accuracy by comparing the predictions with the true labels Y and dividing by the total number of examples\n",
    "\n",
    "#defining the gradient descent function\n",
    "def gradient_descent(X,Y,alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(W1, Z1, A1, W2, A2, Z2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i%10==0:\n",
    "            print(\"iteration: \", i)\n",
    "            predictions= get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return  W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bc20d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "#lets test some predictiosn from our dev dataset\n",
    "def test_predictions(index, W1, b1, W2, b2):\n",
    "    current_image= X_train[:, index, None]\n",
    "    prediction = make_predictions(current_image,W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(f\"Prediction: {prediction}, Actual: {label}\")\n",
    "    \n",
    "    digit= current_image.reshape((28, 28))*255\n",
    "    plt.gray()\n",
    "    plt.imshow(digit, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions(74, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on the dev dataset\n",
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "get_accuracy(dev_predictions, Y_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
