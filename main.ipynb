{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933d3111",
   "metadata": {},
   "source": [
    "A Simple Digit Recognizer from the infamous MNIST dataset. This is actually called the \"Hello World!\" for Neural Networks. The project is taking help from a kaggle note book adn two you tube videos.\n",
    "\n",
    "1. Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math) by Samson Zhang - https://www.youtube.com/watch?v=w8yWXqWQYmU\n",
    "2. Neural networks Series by 3Blue1Brown (First 4 videos are good enough for the project) - https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the needed libraries and connecting to the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data  =pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dd0ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m,n = data.shape\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae8a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There could be a chance the dataset will be ordered, so we shuffle it\n",
    "np.random.shuffle(data) \n",
    "\n",
    "#Splitting the adatset into dev and training sets and transposing it\n",
    "\n",
    "#Dev dataset\n",
    "data_dev=data[1:1000].T\n",
    "Y_dev=data_dev[0]\n",
    "X_dev=data_dev[1:n]\n",
    "# Normalizing the data from 0–255 (grayscale) to 0–1\n",
    "X_dev= X_dev/255.0 \n",
    "\n",
    "#Training dataset\n",
    "data_train=data[1000:m].T\n",
    "Y_train= data_train[0]\n",
    "X_train=data_train[1:n]\n",
    "X_train=X_train/255.0\n",
    "\n",
    "_,m_train=X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f1917d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5e45d",
   "metadata": {},
   "source": [
    "Neural Network Architecture\n",
    "Input Layer a[0]:\n",
    "\n",
    "    784 units\n",
    "\n",
    "    One for each pixel in a 28×28 image\n",
    "\n",
    "Hidden Layer a[1]:\n",
    "\n",
    "    10 units\n",
    "\n",
    "    Uses ReLU activation function\n",
    "\n",
    "Output Layer a[2]:\n",
    "\n",
    "    10 units\n",
    "\n",
    "    One for each digit class (0–9)\n",
    "\n",
    "    Uses Softmax activation to output class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the input parameters or initial weight and biases for the neural network\n",
    "\n",
    "def init_params():\n",
    "    W1=np.random.rand(10,784)-0.5 #to keeep the values between 0.5 and -0.5\n",
    "    b1=np.random.rand(10,1)-0.5\n",
    "    W2=np.random.rand(10,10)-0.5\n",
    "    b2=np.random.rand(10,1)-0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "#Lets define the ReLu activation function\n",
    "def ReLu(Z):\n",
    "    return np.maximum(0, Z) #there is still possibility for making it faster- i will come back for this later\n",
    "\n",
    "#Lets define the softmax function- will help convert the activation values into proobabilities\n",
    "def softmax(Z):\n",
    "    A= np.exp(Z)/sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "#Lets define the forward propogation function\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1= np.dot(W1, X) + b1\n",
    "    A1=ReLu(Z1)\n",
    "    Z2= np.dot(W2, A1) + b2\n",
    "    A2= softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
