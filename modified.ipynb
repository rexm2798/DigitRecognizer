{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d013fe57",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition with Neural Network (from Scratch)\n",
    "This notebook builds a simple neural network using only NumPy, Pandas to classify digits from the MNIST dataset.\n",
    "We’ll walk through data loading, preprocessing, model creation, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872957ed",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "We start by importing NumPy for numerical operations, pandas for data handling, and matplotlib for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92be8522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (42000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb2ac7",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "We load the `train.csv` file which contains labeled images of handwritten digits. Each row represents one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3504e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST training dataset\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(f\"Loaded dataset with shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b951c",
   "metadata": {},
   "source": [
    "## 3. Exploring the Dataset\n",
    "We convert the dataset to a NumPy array and print its dimensions to understand how many examples and features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa03bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 42000 examples with 785 features (including the label).\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to NumPy array and get dataset shape\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "print(f\"Dataset contains {m} examples with {n} features (including the label).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd1010",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "We shuffle the data, normalize pixel values, and split it into training and development sets.\n",
    "Normalization helps the model train faster by scaling pixel values to the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset to avoid any ordering bias\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the dataset into training and dev sets\n",
    "# Transpose so that each column represents one example\n",
    "data_dev = data[1:1000].T\n",
    "X_dev = data_dev[1:] / 255.0  # Normalize pixel values\n",
    "Y_dev = data_dev[0]\n",
    "\n",
    "data_train = data[1000:].T\n",
    "X_train = data_train[1:] / 255.0  # Normalize pixel values\n",
    "Y_train = data_train[0]\n",
    "\n",
    "# Get the number of training examples\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf788c",
   "metadata": {},
   "source": [
    "## 5. Neural Network Implementation\n",
    "\n",
    "### 5.1 Neural Network Architecture Overview\n",
    "We build a simple feedforward neural network with the following structure:\n",
    "- **Input layer**: 784 units (one for each pixel in a 28×28 grayscale image)\n",
    "- **Hidden layer**: 10 neurons with ReLU activation\n",
    "- **Output layer**: 10 neurons with softmax activation, one for each digit class (0–9)\n",
    "---\n",
    "\n",
    "### 5.2 Initializing Parameters\n",
    "We randomly initialize the weights and biases for both layers.\n",
    "This helps break symmetry and allows the network to begin learning.\n",
    "\n",
    "### 5.3 Activation Functions\n",
    "We define the activation functions used in the network:\n",
    "- **ReLU (Rectified Linear Unit)** for the hidden layer\n",
    "- **Softmax** for the output layer to convert scores into probabilities\n",
    "\n",
    "### 5.4 Forward Propagation\n",
    "\n",
    "In forward propagation, we compute activations layer by layer using weights, biases, and activation functions.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{784 \\times m}$: input matrix (each column is an image)\n",
    "- $W^{[1]} \\in \\mathbb{R}^{10 \\times 784}$, $b^{[1]} \\in \\mathbb{R}^{10 \\times 1}$: weights and bias of the first (hidden) layer\n",
    "- $W^{[2]} \\in \\mathbb{R}^{10 \\times 10}$, $b^{[2]} \\in \\mathbb{R}^{10 \\times 1}$: weights and bias of the output layer\n",
    "- $m$: number of training examples\n",
    "\n",
    "#### Step-by-step Computation:\n",
    "\n",
    "**Hidden layer**:\n",
    "\n",
    "$$\n",
    "Z^{[1]} = W^{[1]} X + b^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[1]} = \\text{ReLU}(Z^{[1]}) = \\max(0, Z^{[1]})\n",
    "$$\n",
    "\n",
    "**Output layer**:\n",
    "\n",
    "$$\n",
    "Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[2]} = \\text{softmax}(Z^{[2]}) = \\frac{e^{Z^{[2]}}}{\\sum e^{Z^{[2]}}}\n",
    "$$\n",
    "\n",
    "Softmax is applied column-wise to ensure each column of \\( A^{[2]} \\) sums to 1.\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "If the image is a \"3\", then:\n",
    "\n",
    "$$\n",
    "A^{[2]} = [0.01, 0.02, 0.01, \\mathbf{0.94}, 0.01, \\dots]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 5.5 One-Hot Encoding\n",
    "\n",
    "In a multi-class classification problem like digit recognition (0–9), we need to compare the predicted probabilities with the true labels.  \n",
    "Since labels are scalar integers (e.g., 3, 7), we convert them into **one-hot encoded vectors** for mathematical compatibility during loss calculation and backpropagation.\n",
    "\n",
    "#### Definition:\n",
    "\n",
    "If a label $y = 3$, the corresponding one-hot encoded vector $\\mathbf{y} \\in \\mathbb{R}^{10 \\times 1}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So in general:\n",
    "\n",
    "- If $Y \\in \\mathbb{R}^{1 \\times m}$ contains labels (digits 0–9),\n",
    "- Then one-hot encoding creates $Y_{\\text{one-hot}} \\in \\mathbb{R}^{10 \\times m}$, where each column is a one-hot vector.\n",
    "\n",
    "#### Why it's important:\n",
    "\n",
    "During backpropagation, we subtract the one-hot encoded labels from the predicted softmax probabilities:\n",
    "\n",
    "$$\n",
    "\\delta^{[2]} = A^{[2]} - Y_{\\text{one-hot}}\n",
    "$$\n",
    "\n",
    "This allows us to compute the **cross-entropy gradient** effectively.\n",
    "\n",
    "\n",
    "### 5.6 Backward Propagation\n",
    "\n",
    "Once we compute predictions using forward propagation, we calculate how far off the predictions are from the actual labels. Backward propagation helps us compute the **gradients** of the loss with respect to each parameter so we can update them and reduce the error.\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "We use **cross-entropy loss** for multi-class classification:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{10} Y_j^{(i)} \\log\\left(A_j^{[2](i)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Y^{(i)} \\) is the one-hot encoded true label for the \\(i\\)-th training example\n",
    "- \\( A^{[2](i)} \\) is the predicted softmax output for the \\(i\\)-th example\n",
    "- \\( m \\) is the total number of examples\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradients for Output Layer\n",
    "\n",
    "Let:\n",
    "- \\( A^{[2]} \\in \\mathbb{R}^{10 \\times m} \\): output of the softmax layer\n",
    "- \\( Y \\in \\mathbb{R}^{10 \\times m} \\): one-hot encoded labels\n",
    "\n",
    "Then the error at the output is:\n",
    "\n",
    "$$\n",
    "\\delta^{[2]} = A^{[2]} - Y\n",
    "$$\n",
    "\n",
    "Gradients for weights and biases in the output layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}} = \\frac{1}{m} \\delta^{[2]} (A^{[1]})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta^{[2](i)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradients for Hidden Layer\n",
    "\n",
    "We propagate the error back to the hidden layer:\n",
    "\n",
    "$$\n",
    "\\delta^{[1]} = (W^{[2]})^T \\delta^{[2]} \\circ \\mathbb{1}(Z^{[1]} > 0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\circ \\) denotes element-wise multiplication (Hadamard product)\n",
    "- \\( \\mathbb{1}(Z^{[1]} > 0) \\) is the derivative of ReLU (i.e., 1 where \\( Z^{[1]} > 0 \\), else 0)\n",
    "\n",
    "Gradients for weights and biases in the hidden layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}} = \\frac{1}{m} \\delta^{[1]} X^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta^{[1](i)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Backward propagation allows the network to understand how each parameter contributed to the error and adjust accordingly using gradient descent. This is the core of how the network \"learns\".\n",
    "\n",
    "\n",
    "### 5.7 Updating Parameters\n",
    "Using the gradients computed during backpropagation, we update the weights and biases via gradient descent.\n",
    "\n",
    "### 5.8 Making Predictions and Evaluating Accuracy\n",
    "We define functions to:\n",
    "- Make predictions using the trained model\n",
    "- Evaluate how well the model performs by calculating accuracy on the training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size=784, hidden_size=10, output_size=10):\n",
    "        self.W1 = np.random.rand(hidden_size, input_size) - 0.5\n",
    "        self.b1 = np.random.rand(hidden_size, 1) - 0.5\n",
    "        self.W2 = np.random.rand(output_size, hidden_size) - 0.5\n",
    "        self.b2 = np.random.rand(output_size, 1) - 0.5\n",
    "\n",
    "    def ReLU(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def ReLU_deriv(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        A= np.exp(Z)/sum(np.exp(Z))\n",
    "        return A\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = self.ReLU(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def one_hot(self, Y):\n",
    "        one_hot_Y = np.zeros((Y.max()+1, Y.size))\n",
    "        one_hot_Y[Y, np.arange(Y.size)] = 1\n",
    "        return one_hot_Y\n",
    "\n",
    "    def backward(self, X, Y, Z1, A1, A2):\n",
    "        m = X.shape[1]\n",
    "        one_hot_Y = self.one_hot(Y)\n",
    "        dZ2 = A2 - one_hot_Y\n",
    "        dW2 = 1/m * dZ2 @ A1.T\n",
    "        db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = (self.W2.T @ dZ2) * self.ReLU_deriv(Z1)\n",
    "        dW1 = 1/m * dZ1 @ X.T\n",
    "        db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, alpha):\n",
    "        self.W1 -= alpha * dW1\n",
    "        self.b1 -= alpha * db1\n",
    "        self.W2 -= alpha * dW2\n",
    "        self.b2 -= alpha * db2\n",
    "\n",
    "    def train(self, X, Y, iterations, alpha):\n",
    "        for i in range(iterations):\n",
    "            Z1, A1, Z2, A2 = self.forward(X)\n",
    "            dW1, db1, dW2, db2 = self.backward(X, Y, Z1, A1, A2)\n",
    "            self.update(dW1, db1, dW2, db2, alpha)\n",
    "            if i % 10 == 0:\n",
    "                preds = self.predict(X)\n",
    "                acc = self.accuracy(preds, Y)\n",
    "                print(f\"Iteration {i} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, _, A2 = self.forward(X)\n",
    "        return np.argmax(A2, axis=0)\n",
    "\n",
    "    def accuracy(self, preds, Y):\n",
    "        return np.mean(preds == Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09048f",
   "metadata": {},
   "source": [
    "## 6 Training the Model\n",
    "We run gradient descent over multiple iterations to train the model.\n",
    "During training, we monitor the accuracy to observe learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21079f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.train(X_train, Y_train, iterations=500, alpha=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
