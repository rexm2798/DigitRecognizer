{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d013fe57",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition with Neural Network (from Scratch)\n",
    "This notebook builds a simple neural network using only NumPy, Pandas to classify digits from the MNIST dataset.\n",
    "We’ll walk through data loading, preprocessing, model creation, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872957ed",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "We start by importing NumPy for numerical operations, pandas for data handling, and matplotlib for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be8522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (42000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb2ac7",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "We load the `train.csv` file which contains labeled images of handwritten digits. Each row represents one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3504e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST training dataset\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(f\"Loaded dataset with shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b951c",
   "metadata": {},
   "source": [
    "## 3. Exploring the Dataset\n",
    "We convert the dataset to a NumPy array and print its dimensions to understand how many examples and features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa03bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 42000 examples with 785 features (including the label).\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to NumPy array and get dataset shape\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "print(f\"Dataset contains {m} examples with {n} features (including the label).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd1010",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "We shuffle the data, normalize pixel values, and split it into training and development sets.\n",
    "Normalization helps the model train faster by scaling pixel values to the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset to avoid any ordering bias\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the dataset into training and dev sets\n",
    "# Transpose so that each column represents one example\n",
    "data_dev = data[1:1000].T\n",
    "X_dev = data_dev[1:] / 255.0  # Normalize pixel values\n",
    "Y_dev = data_dev[0]\n",
    "\n",
    "data_train = data[1000:].T\n",
    "X_train = data_train[1:] / 255.0  # Normalize pixel values\n",
    "Y_train = data_train[0]\n",
    "\n",
    "# Get the number of training examples\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf788c",
   "metadata": {},
   "source": [
    "## 5. Neural Network Implementation\n",
    "\n",
    "### 5.1 Neural Network Architecture Overview\n",
    "We build a simple feedforward neural network with the following structure:\n",
    "- **Input layer**: 784 units (one for each pixel in a 28×28 grayscale image)\n",
    "- **Hidden layer**: 10 neurons with ReLU activation\n",
    "- **Output layer**: 10 neurons with softmax activation, one for each digit class (0–9)\n",
    "\n",
    "### 5.2 Initializing Parameters\n",
    "We randomly initialize the weights and biases for both layers.\n",
    "This helps break symmetry and allows the network to begin learning.\n",
    "\n",
    "### 5.3 Activation Functions\n",
    "We define the activation functions used in the network:\n",
    "- **ReLU (Rectified Linear Unit)** for the hidden layer\n",
    "- **Softmax** for the output layer to convert scores into probabilities\n",
    "\n",
    "### 5.4 Forward Propagation\n",
    "We calculate the intermediate activations and output probabilities by applying the forward pass of the network.\n",
    "\n",
    "### 5.5 One-Hot Encoding\n",
    "Since our labels are digits (0–9), we convert them into one-hot encoded vectors to compute the loss and gradients properly.\n",
    "\n",
    "### 5.6 Backward Propagation\n",
    "We compute gradients of the loss with respect to all parameters using the chain rule.\n",
    "This allows us to update the weights and biases to minimize the prediction error.\n",
    "\n",
    "### 5.7 Updating Parameters\n",
    "Using the gradients computed during backpropagation, we update the weights and biases via gradient descent.\n",
    "\n",
    "### 5.8 Making Predictions and Evaluating Accuracy\n",
    "We define functions to:\n",
    "- Make predictions using the trained model\n",
    "- Evaluate how well the model performs by calculating accuracy on the training and dev sets\n",
    "\n",
    "### 5.9 Training the Model\n",
    "We run gradient descent over multiple iterations to train the model.\n",
    "During training, we monitor the accuracy to observe learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size=784, hidden_size=10, output_size=10):\n",
    "        self.W1 = np.random.rand(hidden_size, input_size) - 0.5\n",
    "        self.b1 = np.random.rand(hidden_size, 1) - 0.5\n",
    "        self.W2 = np.random.rand(output_size, hidden_size) - 0.5\n",
    "        self.b2 = np.random.rand(output_size, 1) - 0.5\n",
    "\n",
    "    def ReLU(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def ReLU_deriv(self, Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        A= np.exp(Z)/sum(np.exp(Z))\n",
    "        return A\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = self.ReLU(Z1)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = self.softmax(Z2)\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def one_hot(self, Y):\n",
    "        one_hot_Y = np.zeros((Y.max()+1, Y.size))\n",
    "        one_hot_Y[Y, np.arange(Y.size)] = 1\n",
    "        return one_hot_Y\n",
    "\n",
    "    def backward(self, X, Y, Z1, A1, A2):\n",
    "        m = X.shape[1]\n",
    "        one_hot_Y = self.one_hot(Y)\n",
    "        dZ2 = A2 - one_hot_Y\n",
    "        dW2 = 1/m * dZ2 @ A1.T\n",
    "        db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = (self.W2.T @ dZ2) * self.ReLU_deriv(Z1)\n",
    "        dW1 = 1/m * dZ1 @ X.T\n",
    "        db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, alpha):\n",
    "        self.W1 -= alpha * dW1\n",
    "        self.b1 -= alpha * db1\n",
    "        self.W2 -= alpha * dW2\n",
    "        self.b2 -= alpha * db2\n",
    "\n",
    "    def train(self, X, Y, iterations, alpha):\n",
    "        for i in range(iterations):\n",
    "            Z1, A1, Z2, A2 = self.forward(X)\n",
    "            dW1, db1, dW2, db2 = self.backward(X, Y, Z1, A1, A2)\n",
    "            self.update(dW1, db1, dW2, db2, alpha)\n",
    "            if i % 10 == 0:\n",
    "                preds = self.predict(X)\n",
    "                acc = self.accuracy(preds, Y)\n",
    "                print(f\"Iteration {i} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, _, A2 = self.forward(X)\n",
    "        return np.argmax(A2, axis=0)\n",
    "\n",
    "    def accuracy(self, preds, Y):\n",
    "        return np.mean(preds == Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21079f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.train(X_train, Y_train, iterations=500, alpha=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
